{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results: 169\n",
      "Total results: 169\n",
      "Total results: 169\n",
      "Total results: 133\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pylab as plt \n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "\n",
    "def to_df(mat_db):\n",
    "    \"\"\"Returns one pd.DataFrame per cycle type\"\"\"\n",
    "\n",
    "    # Features common for every cycle\n",
    "    cycles_cols = ['type', 'ambient_temperature', 'time']\n",
    "\n",
    "    # Features monitored during the cycle\n",
    "    features_cols = {\n",
    "        'charge': ['Voltage_measured', 'Current_measured', 'Temperature_measured', \n",
    "                   'Current_charge', 'Voltage_charge', 'Time'],\n",
    "        'discharge': ['Voltage_measured', 'Current_measured', 'Temperature_measured', \n",
    "                      'Current_charge', 'Voltage_charge', 'Time', 'Capacity'],\n",
    "        'impedance': ['Sense_current', 'Battery_current', 'Current_ratio',\n",
    "                      'Battery_impedance', 'Rectified_impedance', 'Re', 'Rct']\n",
    "    }\n",
    "\n",
    "    # Define one pd.DataFrame per cycle type\n",
    "    df = {key: pd.DataFrame() for key in features_cols.keys()}\n",
    "\n",
    "    # Get every cycle\n",
    "    cycles = [[row.flat[0] for row in line] for line in mat_db[0][0][0][0]]\n",
    "\n",
    "    # Get measures for every cycle\n",
    "    for cycle_id, cycle_data in enumerate(cycles):\n",
    "        tmp = pd.DataFrame()\n",
    "\n",
    "        # Data series for every cycle\n",
    "        features_x_cycle = cycle_data[-1]\n",
    "\n",
    "        # Get features for the specific cycle type\n",
    "        features = features_cols[cycle_data[0]]\n",
    "        \n",
    "        for feature, data in zip(features, features_x_cycle):\n",
    "            if len(data[0]) > 1:\n",
    "                # Correct number of records\n",
    "                tmp[feature] = data[0]\n",
    "            else:\n",
    "                # Single value, so assign it to all rows\n",
    "                tmp[feature] = data[0][0]\n",
    "        \n",
    "        # Add columns common to the cycle measurements\n",
    "        tmp['id_cycle'] = cycle_id\n",
    "        for k, col in enumerate(cycles_cols):\n",
    "            tmp[col] = cycle_data[k]\n",
    "        \n",
    "        # Append cycle data to the right pd.DataFrame using pd.concat()\n",
    "        cycle_type = cycle_data[0]\n",
    "        df[cycle_type] = pd.concat([df[cycle_type], tmp], ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def Mat2List(dfs_mat):\n",
    "    # Example usage\n",
    "    dfs_B0005 = to_df(dfs_mat)\n",
    "\n",
    "    df_cycle_charge = dfs_B0005['charge'] #['id_cycle']\n",
    "    df_cycle_dicharge = dfs_B0005['discharge'] #['id_cycle']\n",
    "    \n",
    "    total_result = []\n",
    "\n",
    "    for i in df_cycle_charge['id_cycle'].unique():\n",
    "        # Filter charge data for the current cycle\n",
    "        df = df_cycle_charge[df_cycle_charge['id_cycle'] == i]\n",
    "\n",
    "        # Extract the required columns\n",
    "        temperature = df['Temperature_measured'].tolist() # 이 리스트의 MinMaxScaler를 통해 정규화\n",
    "        current = df['Current_measured'].tolist()\n",
    "        voltage = df['Voltage_measured'].tolist()\n",
    "\n",
    "        # Find corresponding discharge data\n",
    "        dis = df_cycle_dicharge[df_cycle_dicharge['id_cycle'] == i + 1]\n",
    "        \n",
    "        # Fallback to next cycle if discharge data is empty\n",
    "        if dis.empty:\n",
    "            dis = df_cycle_dicharge[df_cycle_dicharge['id_cycle'] == i + 2]\n",
    "\n",
    "        # Calculate the label (mean capacity), handle if still empty\n",
    "        label = dis['Capacity'].mean() if not dis.empty else None\n",
    "        \n",
    "\n",
    "        # Skip if label is None\n",
    "        if label is None:\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "        \n",
    "            #result = [[np.array(temperature), np.array(current), np.array(voltage)], label]\n",
    "            result = [np.array(temperature), np.array(current), np.array(voltage)], label\n",
    "            # result = np.array(np.array(zip(temperature, current, voltage)), label)\n",
    "            total_result.append(result)\n",
    "        del result\n",
    "    # Check the resulting dataset\n",
    "    print(f\"Total results: {len(total_result)}\")\n",
    "\n",
    "    return total_result\n",
    "\n",
    "B0005 = scipy.io.loadmat('./DATA/1. BatteryAgingARC-FY08Q4/B0005.mat')\n",
    "B0006 = scipy.io.loadmat('./DATA/1. BatteryAgingARC-FY08Q4/B0006.mat')\n",
    "B0007 = scipy.io.loadmat('./DATA/1. BatteryAgingARC-FY08Q4/B0007.mat')\n",
    "B0018 = scipy.io.loadmat('./DATA/1. BatteryAgingARC-FY08Q4/B0018.mat')\n",
    "\n",
    "B0005 = B0005['B0005']\n",
    "B0006 = B0006['B0006']\n",
    "B0007 = B0007['B0007']\n",
    "B0018 = B0018['B0018']\n",
    "# Example usage\n",
    "dfs_B0005 = to_df(B0005)\n",
    "dfs_B0006 = to_df(B0006)\n",
    "dfs_B0007 = to_df(B0007)\n",
    "dfs_B0018 = to_df(B0018)\n",
    "\n",
    "batt_list = [B0005,B0006,B0007]\n",
    "\n",
    "df_train = []\n",
    "for i in batt_list:\n",
    "    df_train+=Mat2List(i)\n",
    "\n",
    "df_test = Mat2List(B0018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results: 169\n",
      "Total results: 169\n",
      "Total results: 169\n",
      "Total results: 133\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def to_df(mat_db):\n",
    "    \"\"\"Returns one pd.DataFrame per cycle type.\"\"\"\n",
    "    cycles_cols = ['type', 'ambient_temperature', 'time']\n",
    "    features_cols = {\n",
    "        'charge': ['Voltage_measured', 'Current_measured', 'Temperature_measured', \n",
    "                   'Current_charge', 'Voltage_charge', 'Time'],\n",
    "        'discharge': ['Voltage_measured', 'Current_measured', 'Temperature_measured', \n",
    "                      'Current_charge', 'Voltage_charge', 'Time', 'Capacity'],\n",
    "        'impedance': ['Sense_current', 'Battery_current', 'Current_ratio',\n",
    "                      'Battery_impedance', 'Rectified_impedance', 'Re', 'Rct']\n",
    "    }\n",
    "\n",
    "    df = {key: pd.DataFrame() for key in features_cols.keys()}\n",
    "    cycles = [[row.flat[0] for row in line] for line in mat_db[0][0][0][0]]\n",
    "\n",
    "    for cycle_id, cycle_data in enumerate(cycles):\n",
    "        tmp = pd.DataFrame()\n",
    "        features_x_cycle = cycle_data[-1]\n",
    "        cycle_type = cycle_data[0]\n",
    "\n",
    "        if cycle_type not in features_cols:\n",
    "            continue\n",
    "\n",
    "        features = features_cols[cycle_type]\n",
    "        for feature, data in zip(features, features_x_cycle):\n",
    "            if len(data[0]) > 1:\n",
    "                tmp[feature] = data[0]\n",
    "            else:\n",
    "                tmp[feature] = [data[0][0]] * len(tmp)\n",
    "\n",
    "        tmp['id_cycle'] = cycle_id\n",
    "        for k, col in enumerate(cycles_cols):\n",
    "            tmp[col] = cycle_data[k]\n",
    "\n",
    "        df[cycle_type] = pd.concat([df[cycle_type], tmp], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def Mat2List(dfs_mat):\n",
    "    dfs = to_df(dfs_mat)\n",
    "\n",
    "    df_cycle_charge = dfs['charge']\n",
    "    df_cycle_discharge = dfs['discharge']\n",
    "\n",
    "    total_result = []\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))  # Initialize MinMaxScaler\n",
    "\n",
    "    for i in df_cycle_charge['id_cycle'].unique():\n",
    "        df = df_cycle_charge[df_cycle_charge['id_cycle'] == i]\n",
    "\n",
    "        # Extract required columns and apply MinMax scaling\n",
    "        temperature = np.array(df['Temperature_measured'].tolist()).reshape(-1, 1)\n",
    "        current = np.array(df['Current_measured'].tolist()).reshape(-1, 1)\n",
    "        voltage = np.array(df['Voltage_measured'].tolist()).reshape(-1, 1)\n",
    "\n",
    "        normalized_temperature = scaler.fit_transform(temperature).flatten()\n",
    "        normalized_current = scaler.fit_transform(current).flatten()\n",
    "        normalized_voltage = scaler.fit_transform(voltage).flatten()\n",
    "\n",
    "        # Find corresponding discharge data\n",
    "        dis = df_cycle_discharge[df_cycle_discharge['id_cycle'] == i + 1]\n",
    "        if dis.empty:\n",
    "            dis = df_cycle_discharge[df_cycle_discharge['id_cycle'] == i + 2]\n",
    "\n",
    "        # Calculate label\n",
    "        label = dis['Capacity'].mean() if not dis.empty else None\n",
    "\n",
    "        if label is None:\n",
    "            continue\n",
    "\n",
    "        # result = [[normalized_temperature, normalized_current, normalized_voltage], label]\n",
    "        result = [normalized_temperature, normalized_current, normalized_voltage], label\n",
    "        total_result.append(result)\n",
    "\n",
    "    print(f\"Total results: {len(total_result)}\")\n",
    "    return total_result\n",
    "\n",
    "\n",
    "# Load MATLAB files\n",
    "B0005 = scipy.io.loadmat('./DATA/1. BatteryAgingARC-FY08Q4/B0005.mat')['B0005']\n",
    "B0006 = scipy.io.loadmat('./DATA/1. BatteryAgingARC-FY08Q4/B0006.mat')['B0006']\n",
    "B0007 = scipy.io.loadmat('./DATA/1. BatteryAgingARC-FY08Q4/B0007.mat')['B0007']\n",
    "B0018 = scipy.io.loadmat('./DATA/1. BatteryAgingARC-FY08Q4/B0018.mat')['B0018']\n",
    "\n",
    "# Process data\n",
    "batt_list = [B0005, B0006, B0007]\n",
    "df_train = []\n",
    "for batt in batt_list:\n",
    "    df_train += Mat2List(batt)\n",
    "\n",
    "df_test = Mat2List(B0018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ime203\\AppData\\Local\\Temp\\ipykernel_7380\\1006805831.py:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  sequence = torch.tensor(sequence, dtype=torch.float32).unsqueeze(0)  # Channel dimension 추가\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "# Dataset 정의\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, label = self.data[idx]\n",
    "        sequence = torch.tensor(sequence, dtype=torch.float32).unsqueeze(0)  # Channel dimension 추가\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return sequence, label\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    # 가장 긴 길이 계산 (마지막 차원 기준)\n",
    "    max_length = max(seq.shape[-1] for seq in sequences)\n",
    "    # 각 텐서에 대해 패딩 적용\n",
    "    padded_sequences = [\n",
    "        torch.nn.functional.pad(seq, (0, max_length - seq.shape[-1]))  # 마지막 차원을 기준으로 패딩\n",
    "        for seq in sequences\n",
    "    ]\n",
    "    # 배치를 쌓기\n",
    "    padded_sequences = torch.stack(padded_sequences)\n",
    "    # 레이블 배치 생성\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    return padded_sequences, labels\n",
    "\n",
    "# 데이터 클리닝 함수\n",
    "def clean_data(dataset):\n",
    "    cleaned_data = []\n",
    "    for sequence, label in dataset:\n",
    "        # NaN이 없는 데이터만 추가\n",
    "        if not (torch.isnan(sequence).any() or torch.isnan(label).any()):\n",
    "            cleaned_data.append((sequence.numpy(), label.item()))  # Python 기본 타입으로 변환\n",
    "    return cleaned_data\n",
    "\n",
    "# 샘플 데이터 (train과 val 데이터셋 정의)\n",
    "\n",
    "# Dataset 객체 생성\n",
    "train_dataset_with_nan = SequenceDataset(df_train)\n",
    "val_dataset_with_nan = SequenceDataset(df_test)\n",
    "\n",
    "# NaN 데이터 제거\n",
    "cleaned_train_data = clean_data(train_dataset_with_nan)\n",
    "cleaned_val_data = clean_data(val_dataset_with_nan)\n",
    "\n",
    "# NaN 제거된 데이터셋 래핑\n",
    "cleaned_train_dataset = SequenceDataset(cleaned_train_data)\n",
    "cleaned_val_dataset = SequenceDataset(cleaned_val_data)\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(cleaned_train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(cleaned_val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2.4655e+01,  2.4666e+01,  2.4675e+01,  ...,  2.4519e+01,\n",
       "            2.4514e+01,  2.4507e+01],\n",
       "          [-1.2007e-03, -4.0303e+00,  1.5127e+00,  ..., -3.5059e-04,\n",
       "           -1.8559e-03, -2.8924e-03],\n",
       "          [ 3.8730e+00,  3.4794e+00,  4.0006e+00,  ...,  4.1914e+00,\n",
       "            4.1915e+00,  4.1911e+00]]]),\n",
       " tensor(1.8565))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_with_nan[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "LSTM: Expected input to be 2D or 3D, got 5D instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# 에포크 수\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m features, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     28\u001b[0m         \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, 1)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), labels)  \u001b[38;5;66;03m# (batch_size,)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;66;03m# Backward\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[105], line 10\u001b[0m, in \u001b[0;36mRegressionRNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 10\u001b[0m     _, (hidden, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# hidden -> (1, batch_size, 16)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m hidden\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (batch_size, 16)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)  \u001b[38;5;66;03m# (batch_size, 1)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1074\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1073\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m-> 1074\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1075\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM: Expected input to be 2D or 3D, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1076\u001b[0m         )\n\u001b[0;32m   1077\u001b[0m     is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m   1078\u001b[0m     batch_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: LSTM: Expected input to be 2D or 3D, got 5D instead"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RegressionRNN(nn.Module):\n",
    "    def __init__(self,input_shape):\n",
    "        super(RegressionRNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size=input_shape[0], hidden_size=16, batch_first=True)\n",
    "        self.fc = nn.Linear(16, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.rnn(x)  # hidden -> (1, batch_size, 16)\n",
    "        x = hidden.squeeze(0)  # (batch_size, 16)\n",
    "        x = self.fc(x)  # (batch_size, 1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "import torch.optim as optim\n",
    "input_shape = (1, 3, 3828)  # Example input shape from train_loader\n",
    "\n",
    "\n",
    "# 모델, 손실 함수 및 옵티마이저 정의\n",
    "model = RegressionRNN(input_shape[1:])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(50):  # 에포크 수\n",
    "    for features, labels in train_loader:\n",
    "        # Forward\n",
    "        outputs = model(features)  # (batch_size, 1)\n",
    "        loss = criterion(outputs.squeeze(), labels)  # (batch_size,)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [1, 3, 3828]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     52\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3828\u001b[39m)  \u001b[38;5;66;03m# Example input shape from train_loader\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCNNModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Print model summary\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m, in \u001b[0;36mCNNModel.__init__\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mAdaptiveMaxPool2d((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Calculate the flatten size dynamically\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_flatten_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_size, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 26\u001b[0m, in \u001b[0;36mCNNModel._get_flatten_size\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_flatten_size\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_shape):\n\u001b[0;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39minput_shape)  \u001b[38;5;66;03m# Simulate a batch with size 1\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleaky_relu1(x)\n\u001b[0;32m     28\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(x)\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\conv.py:725\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\conv.py:720\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[0;32m    710\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    711\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    719\u001b[0m     )\n\u001b[1;32m--> 720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 4D (unbatched) or 5D (batched) input to conv3d, but got input of size: [1, 3, 3828]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# CNN 모델 정의\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels=input_shape[0], out_channels=30, kernel_size=(1, 1), stride=(1, 1))\n",
    "        self.leaky_relu1 = nn.LeakyReLU(0.1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=30, out_channels=15, kernel_size=(1, 1), stride=(1, 1))\n",
    "        self.leaky_relu2 = nn.LeakyReLU(0.1)\n",
    "\n",
    "        # Dynamically adjust the pooling size for valid output dimensions\n",
    "        self.pool2 = nn.AdaptiveMaxPool2d((1, 1))\n",
    "\n",
    "        # Calculate the flatten size dynamically\n",
    "        self.flatten_size = self._get_flatten_size(input_shape)\n",
    "\n",
    "        self.fc = nn.Linear(self.flatten_size, 1)\n",
    "\n",
    "    def _get_flatten_size(self, input_shape):\n",
    "        x = torch.zeros(1, *input_shape)  # Simulate a batch with size 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.leaky_relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.leaky_relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        return x.numel()\n",
    "    def forward(self, x):\n",
    "        # Remove unnecessary dimensions (squeeze depth dimension if it's 1)\n",
    "        x = x.squeeze(2)  # Assuming input is [batch_size, channels, depth=1, height, width]\n",
    "        x = self.conv1(x)\n",
    "        x = self.leaky_relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.leaky_relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Example usage\n",
    "input_shape = (1, 3, 3828)  # Example input shape from train_loader\n",
    "model = CNNModel(input_shape[1:]).to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 200\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for features, labels in train_loader:\n",
    "        # 데이터 이동\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))  # Labels 차원 맞추기\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate training loss\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_features, val_labels in val_loader:\n",
    "            val_features, val_labels = val_features.to(device), val_labels.to(device)\n",
    "            val_outputs = model(val_features)\n",
    "            val_loss += criterion(val_outputs, val_labels.unsqueeze(1)).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # Print losses for this epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Testing loop\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for features, labels in val_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        outputs = model(features)\n",
    "        test_loss += criterion(outputs, labels.unsqueeze(1)).item()\n",
    "\n",
    "test_loss /= len(val_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CNNModel(\n",
      "  (conv1): Conv2d(1, 30, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu1): LeakyReLU(negative_slope=0.1)\n",
      "  (pool1): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout1): Dropout(p=0.1, inplace=False)\n",
      "  (conv2): Conv2d(30, 15, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu2): LeakyReLU(negative_slope=0.1)\n",
      "  (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  (pool2): AdaptiveMaxPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=15, out_features=1, bias=True)\n",
      "  (dropout_fc): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Epoch 1/100, Train Loss: 0.1171, Val Loss: 0.3602\n",
      "Epoch 2/100, Train Loss: 0.0718, Val Loss: 0.3226\n",
      "Epoch 3/100, Train Loss: 0.0620, Val Loss: 0.3239\n",
      "Epoch 4/100, Train Loss: 0.0666, Val Loss: 0.3722\n",
      "Epoch 5/100, Train Loss: 0.0619, Val Loss: 0.2773\n",
      "Epoch 6/100, Train Loss: 0.0597, Val Loss: 0.3121\n",
      "Epoch 7/100, Train Loss: 0.0492, Val Loss: 0.2112\n",
      "Epoch 8/100, Train Loss: 0.0496, Val Loss: 0.1973\n",
      "Epoch 9/100, Train Loss: 0.0555, Val Loss: 0.2292\n",
      "Epoch 10/100, Train Loss: 0.0494, Val Loss: 0.1673\n",
      "Epoch 11/100, Train Loss: 0.0474, Val Loss: 0.1140\n",
      "Epoch 12/100, Train Loss: 0.0459, Val Loss: 0.1323\n",
      "Epoch 13/100, Train Loss: 0.0483, Val Loss: 0.1591\n",
      "Epoch 14/100, Train Loss: 0.0433, Val Loss: 0.1280\n",
      "Epoch 15/100, Train Loss: 0.0483, Val Loss: 0.0739\n",
      "Epoch 16/100, Train Loss: 0.0416, Val Loss: 0.0974\n",
      "Epoch 17/100, Train Loss: 0.0396, Val Loss: 0.1135\n",
      "Epoch 18/100, Train Loss: 0.0378, Val Loss: 0.1032\n",
      "Epoch 19/100, Train Loss: 0.0372, Val Loss: 0.0985\n",
      "Epoch 20/100, Train Loss: 0.0371, Val Loss: 0.0803\n",
      "Epoch 21/100, Train Loss: 0.0398, Val Loss: 0.0930\n",
      "Epoch 22/100, Train Loss: 0.0374, Val Loss: 0.1099\n",
      "Epoch 23/100, Train Loss: 0.0390, Val Loss: 0.0921\n",
      "Epoch 24/100, Train Loss: 0.0379, Val Loss: 0.1239\n",
      "Epoch 25/100, Train Loss: 0.0310, Val Loss: 0.0703\n",
      "Epoch 26/100, Train Loss: 0.0367, Val Loss: 0.0666\n",
      "Epoch 27/100, Train Loss: 0.0313, Val Loss: 0.0721\n",
      "Epoch 28/100, Train Loss: 0.0310, Val Loss: 0.0541\n",
      "Epoch 29/100, Train Loss: 0.0316, Val Loss: 0.1069\n",
      "Epoch 30/100, Train Loss: 0.0312, Val Loss: 0.0564\n",
      "Epoch 31/100, Train Loss: 0.0305, Val Loss: 0.0679\n",
      "Epoch 32/100, Train Loss: 0.0340, Val Loss: 0.0731\n",
      "Epoch 33/100, Train Loss: 0.0305, Val Loss: 0.0830\n",
      "Epoch 34/100, Train Loss: 0.0289, Val Loss: 0.0887\n",
      "Epoch 35/100, Train Loss: 0.0315, Val Loss: 0.0496\n",
      "Epoch 36/100, Train Loss: 0.0302, Val Loss: 0.0551\n",
      "Epoch 37/100, Train Loss: 0.0309, Val Loss: 0.0378\n",
      "Epoch 38/100, Train Loss: 0.0302, Val Loss: 0.0412\n",
      "Epoch 39/100, Train Loss: 0.0287, Val Loss: 0.0479\n",
      "Epoch 40/100, Train Loss: 0.0275, Val Loss: 0.0615\n",
      "Epoch 41/100, Train Loss: 0.0273, Val Loss: 0.0421\n",
      "Epoch 42/100, Train Loss: 0.0284, Val Loss: 0.0510\n",
      "Epoch 43/100, Train Loss: 0.0261, Val Loss: 0.0559\n",
      "Epoch 44/100, Train Loss: 0.0263, Val Loss: 0.0364\n",
      "Epoch 45/100, Train Loss: 0.0275, Val Loss: 0.0683\n",
      "Epoch 46/100, Train Loss: 0.0251, Val Loss: 0.0352\n",
      "Epoch 47/100, Train Loss: 0.0265, Val Loss: 0.0438\n",
      "Epoch 48/100, Train Loss: 0.0244, Val Loss: 0.0494\n",
      "Epoch 49/100, Train Loss: 0.0256, Val Loss: 0.0599\n",
      "Epoch 50/100, Train Loss: 0.0270, Val Loss: 0.0625\n",
      "Epoch 51/100, Train Loss: 0.0251, Val Loss: 0.0428\n",
      "Epoch 52/100, Train Loss: 0.0234, Val Loss: 0.0507\n",
      "Epoch 53/100, Train Loss: 0.0255, Val Loss: 0.0453\n",
      "Epoch 54/100, Train Loss: 0.0235, Val Loss: 0.0504\n",
      "Epoch 55/100, Train Loss: 0.0244, Val Loss: 0.0456\n",
      "Epoch 56/100, Train Loss: 0.0241, Val Loss: 0.0358\n",
      "Epoch 57/100, Train Loss: 0.0231, Val Loss: 0.0329\n",
      "Epoch 58/100, Train Loss: 0.0234, Val Loss: 0.0410\n",
      "Epoch 59/100, Train Loss: 0.0244, Val Loss: 0.0356\n",
      "Epoch 60/100, Train Loss: 0.0254, Val Loss: 0.0339\n",
      "Epoch 61/100, Train Loss: 0.0218, Val Loss: 0.0407\n",
      "Epoch 62/100, Train Loss: 0.0233, Val Loss: 0.0407\n",
      "Epoch 63/100, Train Loss: 0.0213, Val Loss: 0.0403\n",
      "Epoch 64/100, Train Loss: 0.0229, Val Loss: 0.0323\n",
      "Epoch 65/100, Train Loss: 0.0218, Val Loss: 0.0484\n",
      "Epoch 66/100, Train Loss: 0.0234, Val Loss: 0.0322\n",
      "Epoch 67/100, Train Loss: 0.0223, Val Loss: 0.0394\n",
      "Epoch 68/100, Train Loss: 0.0212, Val Loss: 0.0374\n",
      "Epoch 69/100, Train Loss: 0.0219, Val Loss: 0.0467\n",
      "Epoch 70/100, Train Loss: 0.0216, Val Loss: 0.0337\n",
      "Epoch 71/100, Train Loss: 0.0241, Val Loss: 0.0338\n",
      "Epoch 72/100, Train Loss: 0.0228, Val Loss: 0.0329\n",
      "Epoch 73/100, Train Loss: 0.0210, Val Loss: 0.0300\n",
      "Epoch 74/100, Train Loss: 0.0213, Val Loss: 0.0294\n",
      "Epoch 75/100, Train Loss: 0.0212, Val Loss: 0.0294\n",
      "Epoch 76/100, Train Loss: 0.0223, Val Loss: 0.0294\n",
      "Epoch 77/100, Train Loss: 0.0217, Val Loss: 0.0316\n",
      "Epoch 78/100, Train Loss: 0.0224, Val Loss: 0.0325\n",
      "Epoch 79/100, Train Loss: 0.0212, Val Loss: 0.0301\n",
      "Epoch 80/100, Train Loss: 0.0197, Val Loss: 0.0319\n",
      "Epoch 81/100, Train Loss: 0.0223, Val Loss: 0.0315\n",
      "Epoch 82/100, Train Loss: 0.0215, Val Loss: 0.0314\n",
      "Epoch 83/100, Train Loss: 0.0193, Val Loss: 0.0289\n",
      "Epoch 84/100, Train Loss: 0.0201, Val Loss: 0.0290\n",
      "Epoch 85/100, Train Loss: 0.0190, Val Loss: 0.0303\n",
      "Epoch 86/100, Train Loss: 0.0197, Val Loss: 0.0301\n",
      "Epoch 87/100, Train Loss: 0.0204, Val Loss: 0.0297\n",
      "Epoch 88/100, Train Loss: 0.0191, Val Loss: 0.0302\n",
      "Epoch 89/100, Train Loss: 0.0203, Val Loss: 0.0288\n",
      "Epoch 90/100, Train Loss: 0.0200, Val Loss: 0.0292\n",
      "Epoch 91/100, Train Loss: 0.0185, Val Loss: 0.0291\n",
      "Epoch 92/100, Train Loss: 0.0192, Val Loss: 0.0306\n",
      "Epoch 93/100, Train Loss: 0.0209, Val Loss: 0.0307\n",
      "Epoch 94/100, Train Loss: 0.0190, Val Loss: 0.0309\n",
      "Epoch 95/100, Train Loss: 0.0200, Val Loss: 0.0287\n",
      "Epoch 96/100, Train Loss: 0.0179, Val Loss: 0.0285\n",
      "Epoch 97/100, Train Loss: 0.0192, Val Loss: 0.0284\n",
      "Epoch 98/100, Train Loss: 0.0188, Val Loss: 0.0288\n",
      "Epoch 99/100, Train Loss: 0.0195, Val Loss: 0.0294\n",
      "Epoch 100/100, Train Loss: 0.0205, Val Loss: 0.0290\n",
      "Test Loss: 0.0290\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# CNN 모델 정의\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=30, kernel_size=(1, 1), stride=(1, 1))\n",
    "        self.leaky_relu1 = nn.LeakyReLU(0.1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))\n",
    "        self.dropout1 = nn.Dropout(0.1)  # Dropout layer with 0.1 rate\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=30, out_channels=15, kernel_size=(1, 1), stride=(1, 1))\n",
    "        self.leaky_relu2 = nn.LeakyReLU(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.1)  # Dropout layer with 0.1 rate\n",
    "\n",
    "        # Dynamically adjust the pooling size for valid output dimensions\n",
    "        self.pool2 = nn.AdaptiveMaxPool2d((1, 1))\n",
    "\n",
    "        # Calculate the flatten size dynamically\n",
    "        self.flatten_size = self._get_flatten_size(input_shape)\n",
    "\n",
    "        self.fc = nn.Linear(self.flatten_size, 1)\n",
    "        self.dropout_fc = nn.Dropout(0.1)  # Dropout before the fully connected layer\n",
    "\n",
    "    def _get_flatten_size(self, input_shape):\n",
    "        x = torch.zeros(1, *input_shape)  # Simulate a batch with size 1\n",
    "        x = self.conv1(x)\n",
    "        x = self.leaky_relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.leaky_relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        return x.numel()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Remove unnecessary dimensions (squeeze depth dimension if it's 1)\n",
    "        x = x.squeeze(2)  # Assuming input is [batch_size, channels, depth=1, height, width]\n",
    "        x = self.conv1(x)\n",
    "        x = self.leaky_relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)  # Apply dropout after first pooling\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.leaky_relu2(x)\n",
    "        x = self.dropout2(x)  # Apply dropout after second activation\n",
    "\n",
    "        x = self.pool2(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.dropout_fc(x)  # Apply dropout before the fully connected layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Example usage\n",
    "input_shape = (1, 1, 3, 3828)  # Example input shape from train_loader\n",
    "model = CNNModel(input_shape[1:]).to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 200\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for features, labels in train_loader:\n",
    "        # 데이터 이동\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))  # Labels 차원 맞추기\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate training loss\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_features, val_labels in val_loader:\n",
    "            val_features, val_labels = val_features.to(device), val_labels.to(device)\n",
    "            val_outputs = model(val_features)\n",
    "            val_loss += criterion(val_outputs, val_labels.unsqueeze(1)).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # Print losses for this epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Testing loop\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for features, labels in val_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        outputs = model(features)\n",
    "        test_loss += criterion(outputs, labels.unsqueeze(1)).item()\n",
    "\n",
    "test_loss /= len(val_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNLSTMModel(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu1): LeakyReLU(negative_slope=0.1)\n",
      "  (pool1): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (leaky_relu2): LeakyReLU(negative_slope=0.1)\n",
      "  (pool2): AdaptiveMaxPool2d(output_size=(1, None))\n",
      "  (lstm): LSTM(16, 16, batch_first=True)\n",
      "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n",
      "Epoch 1/100, Train Loss: 0.4631, Val Loss: 0.0245\n",
      "Epoch 2/100, Train Loss: 0.0457, Val Loss: 0.0270\n",
      "Epoch 3/100, Train Loss: 0.0427, Val Loss: 0.0281\n",
      "Epoch 4/100, Train Loss: 0.0418, Val Loss: 0.0274\n",
      "Epoch 5/100, Train Loss: 0.0439, Val Loss: 0.0277\n",
      "Epoch 6/100, Train Loss: 0.0425, Val Loss: 0.0321\n",
      "Epoch 7/100, Train Loss: 0.0427, Val Loss: 0.0271\n",
      "Epoch 8/100, Train Loss: 0.0428, Val Loss: 0.0275\n",
      "Epoch 9/100, Train Loss: 0.0432, Val Loss: 0.0276\n",
      "Epoch 10/100, Train Loss: 0.0421, Val Loss: 0.0303\n",
      "Epoch 11/100, Train Loss: 0.0434, Val Loss: 0.0262\n",
      "Epoch 12/100, Train Loss: 0.0438, Val Loss: 0.0314\n",
      "Epoch 13/100, Train Loss: 0.0442, Val Loss: 0.0304\n",
      "Epoch 14/100, Train Loss: 0.0429, Val Loss: 0.0286\n",
      "Epoch 15/100, Train Loss: 0.0431, Val Loss: 0.0296\n",
      "Epoch 16/100, Train Loss: 0.0427, Val Loss: 0.0268\n",
      "Epoch 17/100, Train Loss: 0.0431, Val Loss: 0.0263\n",
      "Epoch 18/100, Train Loss: 0.0436, Val Loss: 0.0255\n",
      "Epoch 19/100, Train Loss: 0.0430, Val Loss: 0.0277\n",
      "Epoch 20/100, Train Loss: 0.0417, Val Loss: 0.0285\n",
      "Epoch 21/100, Train Loss: 0.0425, Val Loss: 0.0280\n",
      "Epoch 22/100, Train Loss: 0.0448, Val Loss: 0.0273\n",
      "Epoch 23/100, Train Loss: 0.0416, Val Loss: 0.0263\n",
      "Epoch 24/100, Train Loss: 0.0440, Val Loss: 0.0265\n",
      "Epoch 25/100, Train Loss: 0.0414, Val Loss: 0.0283\n",
      "Epoch 26/100, Train Loss: 0.0427, Val Loss: 0.0350\n",
      "Epoch 27/100, Train Loss: 0.0414, Val Loss: 0.0289\n",
      "Epoch 28/100, Train Loss: 0.0434, Val Loss: 0.0276\n",
      "Epoch 29/100, Train Loss: 0.0441, Val Loss: 0.0261\n",
      "Epoch 30/100, Train Loss: 0.0419, Val Loss: 0.0286\n",
      "Epoch 31/100, Train Loss: 0.0419, Val Loss: 0.0278\n",
      "Epoch 32/100, Train Loss: 0.0429, Val Loss: 0.0271\n",
      "Epoch 33/100, Train Loss: 0.0440, Val Loss: 0.0258\n",
      "Epoch 34/100, Train Loss: 0.0422, Val Loss: 0.0273\n",
      "Epoch 35/100, Train Loss: 0.0419, Val Loss: 0.0287\n",
      "Epoch 36/100, Train Loss: 0.0427, Val Loss: 0.0296\n",
      "Epoch 37/100, Train Loss: 0.0432, Val Loss: 0.0267\n",
      "Epoch 38/100, Train Loss: 0.0430, Val Loss: 0.0257\n",
      "Epoch 39/100, Train Loss: 0.0421, Val Loss: 0.0301\n",
      "Epoch 40/100, Train Loss: 0.0416, Val Loss: 0.0282\n",
      "Epoch 41/100, Train Loss: 0.0426, Val Loss: 0.0297\n",
      "Epoch 42/100, Train Loss: 0.0419, Val Loss: 0.0339\n",
      "Epoch 43/100, Train Loss: 0.0416, Val Loss: 0.0261\n",
      "Epoch 44/100, Train Loss: 0.0414, Val Loss: 0.0405\n",
      "Epoch 45/100, Train Loss: 0.0431, Val Loss: 0.0316\n",
      "Epoch 46/100, Train Loss: 0.0435, Val Loss: 0.0259\n",
      "Epoch 47/100, Train Loss: 0.0428, Val Loss: 0.0326\n",
      "Epoch 48/100, Train Loss: 0.0435, Val Loss: 0.0280\n",
      "Epoch 49/100, Train Loss: 0.0416, Val Loss: 0.0270\n",
      "Epoch 50/100, Train Loss: 0.0422, Val Loss: 0.0340\n",
      "Epoch 51/100, Train Loss: 0.0423, Val Loss: 0.0345\n",
      "Epoch 52/100, Train Loss: 0.0413, Val Loss: 0.0301\n",
      "Epoch 53/100, Train Loss: 0.0421, Val Loss: 0.0269\n",
      "Epoch 54/100, Train Loss: 0.0422, Val Loss: 0.0260\n",
      "Epoch 55/100, Train Loss: 0.0412, Val Loss: 0.0265\n",
      "Epoch 56/100, Train Loss: 0.0415, Val Loss: 0.0264\n",
      "Epoch 57/100, Train Loss: 0.0424, Val Loss: 0.0255\n",
      "Epoch 58/100, Train Loss: 0.0413, Val Loss: 0.0273\n",
      "Epoch 59/100, Train Loss: 0.0409, Val Loss: 0.0303\n",
      "Epoch 60/100, Train Loss: 0.0420, Val Loss: 0.0297\n",
      "Epoch 61/100, Train Loss: 0.0421, Val Loss: 0.0260\n",
      "Epoch 62/100, Train Loss: 0.0418, Val Loss: 0.0265\n",
      "Epoch 63/100, Train Loss: 0.0400, Val Loss: 0.0280\n",
      "Epoch 64/100, Train Loss: 0.0406, Val Loss: 0.0256\n",
      "Epoch 65/100, Train Loss: 0.0414, Val Loss: 0.0261\n",
      "Epoch 66/100, Train Loss: 0.0404, Val Loss: 0.0294\n",
      "Epoch 67/100, Train Loss: 0.0414, Val Loss: 0.0257\n",
      "Epoch 68/100, Train Loss: 0.0419, Val Loss: 0.0275\n",
      "Epoch 69/100, Train Loss: 0.0415, Val Loss: 0.0249\n",
      "Epoch 70/100, Train Loss: 0.0420, Val Loss: 0.0288\n",
      "Epoch 71/100, Train Loss: 0.0426, Val Loss: 0.0309\n",
      "Epoch 72/100, Train Loss: 0.0420, Val Loss: 0.0252\n",
      "Epoch 73/100, Train Loss: 0.0407, Val Loss: 0.0258\n",
      "Epoch 74/100, Train Loss: 0.0421, Val Loss: 0.0267\n",
      "Epoch 75/100, Train Loss: 0.0418, Val Loss: 0.0268\n",
      "Epoch 76/100, Train Loss: 0.0413, Val Loss: 0.0263\n",
      "Epoch 77/100, Train Loss: 0.0424, Val Loss: 0.0252\n",
      "Epoch 78/100, Train Loss: 0.0417, Val Loss: 0.0258\n",
      "Epoch 79/100, Train Loss: 0.0433, Val Loss: 0.0264\n",
      "Epoch 80/100, Train Loss: 0.0411, Val Loss: 0.0277\n",
      "Epoch 81/100, Train Loss: 0.0403, Val Loss: 0.0261\n",
      "Epoch 82/100, Train Loss: 0.0398, Val Loss: 0.0274\n",
      "Epoch 83/100, Train Loss: 0.0417, Val Loss: 0.0253\n",
      "Epoch 84/100, Train Loss: 0.0407, Val Loss: 0.0276\n",
      "Epoch 85/100, Train Loss: 0.0406, Val Loss: 0.0446\n",
      "Epoch 86/100, Train Loss: 0.0425, Val Loss: 0.0298\n",
      "Epoch 87/100, Train Loss: 0.0414, Val Loss: 0.0260\n",
      "Epoch 88/100, Train Loss: 0.0403, Val Loss: 0.0291\n",
      "Epoch 89/100, Train Loss: 0.0408, Val Loss: 0.0260\n",
      "Epoch 90/100, Train Loss: 0.0405, Val Loss: 0.0260\n",
      "Epoch 91/100, Train Loss: 0.0418, Val Loss: 0.0262\n",
      "Epoch 92/100, Train Loss: 0.0404, Val Loss: 0.0261\n",
      "Epoch 93/100, Train Loss: 0.0407, Val Loss: 0.0265\n",
      "Epoch 94/100, Train Loss: 0.0412, Val Loss: 0.0299\n",
      "Epoch 95/100, Train Loss: 0.0424, Val Loss: 0.0243\n",
      "Epoch 96/100, Train Loss: 0.0409, Val Loss: 0.0260\n",
      "Epoch 97/100, Train Loss: 0.0399, Val Loss: 0.0351\n",
      "Epoch 98/100, Train Loss: 0.0407, Val Loss: 0.0243\n",
      "Epoch 99/100, Train Loss: 0.0416, Val Loss: 0.0256\n",
      "Epoch 100/100, Train Loss: 0.0407, Val Loss: 0.0258\n",
      "Test Loss: 0.0258\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# CNN + LSTM 모델 정의\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], out_channels=16, kernel_size=(1, 1), stride=(1, 1))\n",
    "        self.leaky_relu1 = nn.LeakyReLU(0.1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1))\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(1, 1), stride=(1, 1))\n",
    "        self.leaky_relu2 = nn.LeakyReLU(0.1)\n",
    "        self.pool2 = nn.AdaptiveMaxPool2d((1, None))  # 유지된 sequence dimension\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=16, hidden_size=16, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 5D 텐서 입력을 4D로 변환\n",
    "        x = x.squeeze(2)  # Remove depth dimension -> (batch_size, channels, height, width)\n",
    "\n",
    "        # CNN 통과\n",
    "        x = self.conv1(x)\n",
    "        x = self.leaky_relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.leaky_relu2(x)\n",
    "        x = self.pool2(x)  # Output shape: (batch_size, channels=15, height=1, sequence_length)\n",
    "\n",
    "        # 차원 변환: (batch_size, sequence_length, features)\n",
    "        x = x.squeeze(2).permute(0, 2, 1)  # Remove height dimension and permute\n",
    "\n",
    "        # LSTM 통과\n",
    "        lstm_out, _ = self.lstm(x)  # -> (batch_size, sequence_length, hidden_size)\n",
    "        x = lstm_out[:, -1, :]  # 마지막 time step의 hidden state 사용\n",
    "\n",
    "        # FC 계층\n",
    "        x = self.fc(x)  # -> (batch_size, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_shape = (1, 1, 3, 3828)  # Example input shape from train_loader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNLSTMModel(input_shape[1:]).to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 100\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Calculate training loss\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_features, val_labels in val_loader:\n",
    "            val_features, val_labels = val_features.to(device), val_labels.to(device)\n",
    "            val_outputs = model(val_features)\n",
    "            val_loss += criterion(val_outputs, val_labels.unsqueeze(1)).item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # Print losses for this epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Testing loop\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for features, labels in val_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        outputs = model(features)\n",
    "        test_loss += criterion(outputs, labels.unsqueeze(1)).item()\n",
    "\n",
    "test_loss /= len(val_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss (MSE): 0.0257\n",
      "Test RMSE: 0.1604\n",
      "Test MAPE: 8.43%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# Testing loop with RMSE and MAPE calculation\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in val_loader:\n",
    "        # Move data to the same device as the model\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "\n",
    "        # Collect predictions and actual values\n",
    "        predictions.extend(outputs.view(-1).tolist())\n",
    "        actuals.extend(labels.view(-1).tolist())\n",
    "\n",
    "        # Compute batch loss\n",
    "        test_loss += criterion(outputs, labels.unsqueeze(1)).item()\n",
    "\n",
    "# Compute final RMSE\n",
    "test_loss /= len(val_loader)\n",
    "rmse = math.sqrt(sum((p - a) ** 2 for p, a in zip(predictions, actuals)) / len(actuals))\n",
    "\n",
    "# Compute MAPE\n",
    "mape = sum(abs((p - a) / a) for p, a in zip(predictions, actuals) if a != 0) / len(actuals) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "print(f\"Test MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test Loss (MSE): 0.2022\n",
    "Test RMSE: 0.4496\n",
    "Test MAPE: 26.73%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# Testing loop with RMSE and MAPE calculation\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, labels in val_loader:\n",
    "        # Move data to the same device as the model\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        features = features.squeeze(2)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "\n",
    "        # Collect predictions and actual values\n",
    "        predictions.extend(outputs.view(-1).tolist())\n",
    "        actuals.extend(labels.view(-1).tolist())\n",
    "\n",
    "        # Compute batch loss\n",
    "        test_loss += criterion(outputs, labels.unsqueeze(1)).item()\n",
    "\n",
    "# Compute final RMSE\n",
    "test_loss /= len(val_loader)\n",
    "rmse = math.sqrt(sum((p - a) ** 2 for p, a in zip(predictions, actuals)) / len(actuals))\n",
    "\n",
    "# Compute MAPE\n",
    "mape = sum(abs((p - a) / a) for p, a in zip(predictions, actuals) if a != 0) / len(actuals) * 100\n",
    "\n",
    "# Print results\n",
    "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "print(f\"Test MAPE: {mape:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "liion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
