{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results: 169\n",
      "Total results: 169\n",
      "Total results: 169\n",
      "Total results: 133\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pylab as plt \n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "\n",
    "def to_df(mat_db):\n",
    "    \"\"\"Returns one pd.DataFrame per cycle type\"\"\"\n",
    "\n",
    "    # Features common for every cycle\n",
    "    cycles_cols = ['type', 'ambient_temperature', 'time']\n",
    "\n",
    "    # Features monitored during the cycle\n",
    "    features_cols = {\n",
    "        'charge': ['Voltage_measured', 'Current_measured', 'Temperature_measured', \n",
    "                   'Current_charge', 'Voltage_charge', 'Time'],\n",
    "        'discharge': ['Voltage_measured', 'Current_measured', 'Temperature_measured', \n",
    "                      'Current_charge', 'Voltage_charge', 'Time', 'Capacity'],\n",
    "        'impedance': ['Sense_current', 'Battery_current', 'Current_ratio',\n",
    "                      'Battery_impedance', 'Rectified_impedance', 'Re', 'Rct']\n",
    "    }\n",
    "\n",
    "    # Define one pd.DataFrame per cycle type\n",
    "    df = {key: pd.DataFrame() for key in features_cols.keys()}\n",
    "\n",
    "    # Get every cycle\n",
    "    cycles = [[row.flat[0] for row in line] for line in mat_db[0][0][0][0]]\n",
    "\n",
    "    # Get measures for every cycle\n",
    "    for cycle_id, cycle_data in enumerate(cycles):\n",
    "        tmp = pd.DataFrame()\n",
    "\n",
    "        # Data series for every cycle\n",
    "        features_x_cycle = cycle_data[-1]\n",
    "\n",
    "        # Get features for the specific cycle type\n",
    "        features = features_cols[cycle_data[0]]\n",
    "        \n",
    "        for feature, data in zip(features, features_x_cycle):\n",
    "            if len(data[0]) > 1:\n",
    "                # Correct number of records\n",
    "                tmp[feature] = data[0]\n",
    "            else:\n",
    "                # Single value, so assign it to all rows\n",
    "                tmp[feature] = data[0][0]\n",
    "        \n",
    "        # Add columns common to the cycle measurements\n",
    "        tmp['id_cycle'] = cycle_id\n",
    "        for k, col in enumerate(cycles_cols):\n",
    "            tmp[col] = cycle_data[k]\n",
    "        \n",
    "        # Append cycle data to the right pd.DataFrame using pd.concat()\n",
    "        cycle_type = cycle_data[0]\n",
    "        df[cycle_type] = pd.concat([df[cycle_type], tmp], ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "B0005 = scipy.io.loadmat('./DATA/1. BatteryAgingARC-FY08Q4/B0005.mat')\n",
    "B0006 = scipy.io.loadmat('./DATA/1. BatteryAgingARC-FY08Q4/B0006.mat')\n",
    "B0007 = scipy.io.loadmat('./DATA/1. BatteryAgingARC-FY08Q4/B0007.mat')\n",
    "B0018 = scipy.io.loadmat('./DATA/1. BatteryAgingARC-FY08Q4/B0018.mat')\n",
    "\n",
    "B0005 = B0005['B0005']\n",
    "B0006 = B0006['B0006']\n",
    "B0007 = B0007['B0007']\n",
    "B0018 = B0018['B0018']\n",
    "# Example usage\n",
    "dfs_B0005 = to_df(B0005)\n",
    "dfs_B0006 = to_df(B0006)\n",
    "dfs_B0007 = to_df(B0007)\n",
    "dfs_B0018 = to_df(B0018)\n",
    "\n",
    "\n",
    "def Mat2List(dfs_mat):\n",
    "    # Example usage\n",
    "    dfs_B0005 = to_df(dfs_mat)\n",
    "\n",
    "    df_cycle_charge = dfs_B0005['charge'] #['id_cycle']\n",
    "    df_cycle_dicharge = dfs_B0005['discharge'] #['id_cycle']\n",
    "    \n",
    "    total_result = []\n",
    "\n",
    "    for i in df_cycle_charge['id_cycle'].unique():\n",
    "        # Filter charge data for the current cycle\n",
    "        df = df_cycle_charge[df_cycle_charge['id_cycle'] == i]\n",
    "\n",
    "        # Extract the required columns\n",
    "        temperature = df['Temperature_measured'].tolist()\n",
    "        current = df['Current_measured'].tolist()\n",
    "        voltage = df['Voltage_measured'].tolist()\n",
    "\n",
    "        # Find corresponding discharge data\n",
    "        dis = df_cycle_dicharge[df_cycle_dicharge['id_cycle'] == i + 1]\n",
    "        \n",
    "        # Fallback to next cycle if discharge data is empty\n",
    "        if dis.empty:\n",
    "            dis = df_cycle_dicharge[df_cycle_dicharge['id_cycle'] == i + 2]\n",
    "\n",
    "        # Calculate the label (mean capacity), handle if still empty\n",
    "        label = dis['Capacity'].mean() if not dis.empty else None\n",
    "\n",
    "        # Skip if label is None\n",
    "        if label is None:\n",
    "            continue\n",
    "\n",
    "        # Create the feature-label tuple\n",
    "        # result = [list(zip(temperature, current, voltage)), label]\n",
    "        result = [[temperature, current, voltage], label]\n",
    "        # result = np.array(np.array(zip(temperature, current, voltage)), label)\n",
    "        total_result.append(result)\n",
    "\n",
    "    # Check the resulting dataset\n",
    "    print(f\"Total results: {len(total_result)}\")\n",
    "\n",
    "    return total_result\n",
    "\n",
    "\n",
    "batt_list = [B0005,B0006,B0007]\n",
    "# batt_list = [B0005,B0006,]\n",
    "df_train = []\n",
    "for i in batt_list:\n",
    "    df_train+=Mat2List(i)\n",
    "\n",
    "df_test = Mat2List(B0018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (16x1925x1). Calculated output size: (16x962x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 153\u001b[0m\n\u001b[0;32m    150\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# 모델 평가\u001b[39;00m\n\u001b[0;32m    156\u001b[0m rmse, mape \u001b[38;5;241m=\u001b[39m evaluate_model(model, val_loader, device)\n",
      "Cell \u001b[1;32mIn[26], line 82\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[0;32m     80\u001b[0m sequences, labels \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     81\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 82\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), labels)\n\u001b[0;32m     84\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[26], line 66\u001b[0m, in \u001b[0;36mConv3D2D_LSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     64\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3d(x)  \u001b[38;5;66;03m# (batch_size, conv3d_out_channels, 1, reduced_feature_dim, reduced_seq_length)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# Remove the depth dimension\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, conv2d_out_channels, reduced_height, reduced_width)\u001b[39;00m\n\u001b[0;32m     67\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Prepare for LSTM: (batch_size, seq_length, feature_dim)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(x)  \u001b[38;5;66;03m# LSTM output: (batch_size, seq_length, lstm_hidden_dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:213\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\functional.py:830\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given input size: (16x1925x1). Calculated output size: (16x962x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# 데이터셋 정의\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, label = self.data[idx]\n",
    "        sequence = torch.tensor(sequence, dtype=torch.float32).unsqueeze(0)  # Add channel dimension\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return sequence, label\n",
    "\n",
    "\n",
    "# NaN 값 제거\n",
    "def clean_data(data):\n",
    "    return [[seq, label] for seq, label in data if not (np.isnan(seq).any() or np.isnan(label))]\n",
    "\n",
    "\n",
    "# 패딩을 적용하는 collate_fn\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    max_length = max(seq.shape[2] for seq in sequences)  # 최대 길이 계산 (seq_length 위치)\n",
    "    padded_sequences = [\n",
    "        torch.nn.functional.pad(seq, (0, max_length - seq.shape[2], 0, 0))  # Feature_dim 위치 고려\n",
    "        for seq in sequences\n",
    "    ]\n",
    "    padded_sequences = torch.stack(padded_sequences)  # (batch_size, channels, seq_length, feature_dim)\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    return padded_sequences, labels\n",
    "\n",
    "\n",
    "# 모델 정의\n",
    "class Conv3D2D_LSTM(nn.Module):\n",
    "    def __init__(self, input_channels, conv3d_out_channels, conv2d_out_channels,\n",
    "                 lstm_hidden_dim, lstm_num_layers, output_dim, dropout=0.1):\n",
    "        super(Conv3D2D_LSTM, self).__init__()\n",
    "        self.conv3d = nn.Sequential(\n",
    "            nn.Conv3d(input_channels, conv3d_out_channels, kernel_size=(3, 3, 3), padding=(1, 1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1, 2, 2))  # Adjust pooling size to match dimensions\n",
    "        )\n",
    "        self.conv2d = nn.Sequential(\n",
    "            nn.Conv2d(conv3d_out_channels, conv2d_out_channels, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=conv2d_out_channels, hidden_size=lstm_hidden_dim,\n",
    "                            num_layers=lstm_num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, channels, seq_length, feature_dim)\n",
    "        x = x.permute(0, 1, 3, 2)  # (batch_size, channels, feature_dim, seq_length)\n",
    "        x = x.unsqueeze(2)  # Add depth dimension for Conv3D\n",
    "        x = self.conv3d(x)  # (batch_size, conv3d_out_channels, 1, reduced_feature_dim, reduced_seq_length)\n",
    "        x = x.squeeze(2)  # Remove the depth dimension\n",
    "        x = self.conv2d(x)  # (batch_size, conv2d_out_channels, reduced_height, reduced_width)\n",
    "        x = x.flatten(start_dim=2).permute(0, 2, 1)  # Prepare for LSTM: (batch_size, seq_length, feature_dim)\n",
    "        x, _ = self.lstm(x)  # LSTM output: (batch_size, seq_length, lstm_hidden_dim)\n",
    "        x = self.fc(x[:, -1, :])  # Use the last time step's output\n",
    "        return x\n",
    "\n",
    "\n",
    "# 모델 학습 함수\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for sequences, labels in train_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in val_loader:\n",
    "                sequences, labels = sequences.to(device), labels.to(device)\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs.squeeze(), labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "\n",
    "# 모델 평가 함수\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in data_loader:\n",
    "            sequences, labels = sequences.to(device), labels.to(device)\n",
    "            outputs = model(sequences)\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    predictions, true_labels = np.array(predictions), np.array(true_labels)\n",
    "    rmse = np.sqrt(np.mean((predictions - true_labels) ** 2))\n",
    "    mape = np.mean(np.abs((true_labels - predictions) / true_labels)) * 100\n",
    "    return rmse, mape\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "input_channels = 1\n",
    "conv3d_out_channels = 8\n",
    "conv2d_out_channels = 16\n",
    "lstm_hidden_dim = 32\n",
    "lstm_num_layers = 2\n",
    "output_dim = 1\n",
    "dropout = 0.1\n",
    "num_epochs = 20\n",
    "learning_rate = 1e-3\n",
    "batch_size = 5\n",
    "\n",
    "# 장치 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "train_data = clean_data(df_train)\n",
    "val_data = clean_data(df_test)\n",
    "\n",
    "train_dataset = SequenceDataset(train_data)\n",
    "val_dataset = SequenceDataset(val_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# 모델, 손실 함수 및 최적화기 초기화\n",
    "model = Conv3D2D_LSTM(input_channels, conv3d_out_channels, conv2d_out_channels,\n",
    "                      lstm_hidden_dim, lstm_num_layers, output_dim, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 모델 학습\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "# 모델 평가\n",
    "rmse, mape = evaluate_model(model, val_loader, device)\n",
    "print(f\"RMSE: {rmse:.4f}, MAPE: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ime203\\AppData\\Local\\Temp\\ipykernel_17004\\59024279.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequence, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (8x1x3x3843). Calculated output size: (8x0x1x1921). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 학습 및 평가\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 평가\u001b[39;00m\n\u001b[0;32m      5\u001b[0m rmse, mape \u001b[38;5;241m=\u001b[39m evaluate_model(model, val_loader, device)\n",
      "Cell \u001b[1;32mIn[20], line 103\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[0;32m    101\u001b[0m sequences, labels \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    102\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 103\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m    105\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[19], line 194\u001b[0m, in \u001b[0;36mConv3D2D_LSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    192\u001b[0m batch_size, channels, seq_length, feature_dim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m    193\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (batch_size, channels, 1, seq_length, feature_dim)\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, conv3d_out_channels, 1, reduced_seq_length, reduced_feature_dim)\u001b[39;00m\n\u001b[0;32m    195\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (batch_size, conv3d_out_channels, reduced_seq_length, reduced_feature_dim)\u001b[39;00m\n\u001b[0;32m    196\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# (batch_size, reduced_seq_length, conv3d_out_channels, reduced_feature_dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:296\u001b[0m, in \u001b[0;36mMaxPool3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ime203\\anaconda3\\envs\\liion\\lib\\site-packages\\torch\\nn\\functional.py:920\u001b[0m, in \u001b[0;36m_max_pool3d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    919\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 920\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool3d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given input size: (8x1x3x3843). Calculated output size: (8x0x1x1921). Output size is too small"
     ]
    }
   ],
   "source": [
    "# 학습 및 평가\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "# 평가\n",
    "rmse, mape = evaluate_model(model, val_loader, device)\n",
    "print(f\"RMSE: {rmse:.4f}, MAPE: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([5, 1, 3, 3900])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ime203\\AppData\\Local\\Temp\\ipykernel_17004\\59024279.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequence, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "for sequences, labels in train_loader:\n",
    "    print(f\"Input shape: {sequences.shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Conv3D2D_LSTM.__init__() missing 2 required positional arguments: 'lstm_num_layers' and 'output_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 모델 초기화\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mConv3D2D_LSTM\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate)\n",
      "\u001b[1;31mTypeError\u001b[0m: Conv3D2D_LSTM.__init__() missing 2 required positional arguments: 'lstm_num_layers' and 'output_dim'"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "input_dim = 3  # 각 데이터 포인트의 차원\n",
    "hidden_dim = 16\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 모델 초기화\n",
    "model = Conv3D2D_LSTM(input_dim, hidden_dim, num_layers, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 학습\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "# 평가\n",
    "rmse, mape = evaluate_model(model, val_loader, device)\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ime203\\AppData\\Local\\Temp\\ipykernel_17244\\2263435769.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sequence, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n",
      "C:\\Users\\ime203\\AppData\\Local\\Temp\\ipykernel_17244\\2263435769.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sequences = [torch.tensor(seq, dtype=torch.float32) for seq in sequences]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train Loss: 1.1272, Val Loss: 0.0257\n",
      "Epoch [2/200], Train Loss: 0.0455, Val Loss: 0.0255\n",
      "Epoch [3/200], Train Loss: 0.0450, Val Loss: 0.0253\n",
      "Epoch [4/200], Train Loss: 0.0447, Val Loss: 0.0270\n",
      "Epoch [5/200], Train Loss: 0.0442, Val Loss: 0.0244\n",
      "Epoch [6/200], Train Loss: 0.0461, Val Loss: 0.0246\n",
      "Epoch [7/200], Train Loss: 0.0448, Val Loss: 0.0261\n",
      "Epoch [8/200], Train Loss: 0.0454, Val Loss: 0.0243\n",
      "Epoch [9/200], Train Loss: 0.0449, Val Loss: 0.0253\n",
      "Epoch [10/200], Train Loss: 0.0452, Val Loss: 0.0315\n",
      "Epoch [11/200], Train Loss: 0.0460, Val Loss: 0.0307\n",
      "Epoch [12/200], Train Loss: 0.0441, Val Loss: 0.0265\n",
      "Epoch [13/200], Train Loss: 0.0442, Val Loss: 0.0254\n",
      "Epoch [14/200], Train Loss: 0.0450, Val Loss: 0.0299\n",
      "Epoch [15/200], Train Loss: 0.0456, Val Loss: 0.0275\n",
      "Epoch [16/200], Train Loss: 0.0457, Val Loss: 0.0242\n",
      "Epoch [17/200], Train Loss: 0.0454, Val Loss: 0.0242\n",
      "Epoch [18/200], Train Loss: 0.0462, Val Loss: 0.0255\n",
      "Epoch [19/200], Train Loss: 0.0450, Val Loss: 0.0276\n",
      "Epoch [20/200], Train Loss: 0.0448, Val Loss: 0.0271\n",
      "Epoch [21/200], Train Loss: 0.0458, Val Loss: 0.0263\n",
      "Epoch [22/200], Train Loss: 0.0445, Val Loss: 0.0243\n",
      "Epoch [23/200], Train Loss: 0.0462, Val Loss: 0.0255\n",
      "Epoch [24/200], Train Loss: 0.0459, Val Loss: 0.0253\n",
      "Epoch [25/200], Train Loss: 0.0451, Val Loss: 0.0270\n",
      "Epoch [26/200], Train Loss: 0.0460, Val Loss: 0.0243\n",
      "Epoch [27/200], Train Loss: 0.0451, Val Loss: 0.0273\n",
      "Epoch [28/200], Train Loss: 0.0452, Val Loss: 0.0243\n",
      "Epoch [29/200], Train Loss: 0.0461, Val Loss: 0.0254\n",
      "Epoch [30/200], Train Loss: 0.0450, Val Loss: 0.0244\n",
      "Epoch [31/200], Train Loss: 0.0450, Val Loss: 0.0252\n",
      "Epoch [32/200], Train Loss: 0.0453, Val Loss: 0.0262\n",
      "Epoch [33/200], Train Loss: 0.0450, Val Loss: 0.0255\n",
      "Epoch [34/200], Train Loss: 0.0451, Val Loss: 0.0244\n",
      "Epoch [35/200], Train Loss: 0.0457, Val Loss: 0.0246\n",
      "Epoch [36/200], Train Loss: 0.0457, Val Loss: 0.0269\n",
      "Epoch [37/200], Train Loss: 0.0452, Val Loss: 0.0295\n",
      "Epoch [38/200], Train Loss: 0.0452, Val Loss: 0.0255\n",
      "Epoch [39/200], Train Loss: 0.0452, Val Loss: 0.0278\n",
      "Epoch [40/200], Train Loss: 0.0453, Val Loss: 0.0243\n",
      "Epoch [41/200], Train Loss: 0.0449, Val Loss: 0.0257\n",
      "Epoch [42/200], Train Loss: 0.0449, Val Loss: 0.0251\n",
      "Epoch [43/200], Train Loss: 0.0444, Val Loss: 0.0244\n",
      "Epoch [44/200], Train Loss: 0.0451, Val Loss: 0.0272\n",
      "Epoch [45/200], Train Loss: 0.0448, Val Loss: 0.0242\n",
      "Epoch [46/200], Train Loss: 0.0448, Val Loss: 0.0261\n",
      "Epoch [47/200], Train Loss: 0.0436, Val Loss: 0.0333\n",
      "Epoch [48/200], Train Loss: 0.0457, Val Loss: 0.0253\n",
      "Epoch [49/200], Train Loss: 0.0452, Val Loss: 0.0246\n",
      "Epoch [50/200], Train Loss: 0.0446, Val Loss: 0.0244\n",
      "Epoch [51/200], Train Loss: 0.0454, Val Loss: 0.0295\n",
      "Epoch [52/200], Train Loss: 0.0444, Val Loss: 0.0284\n",
      "Epoch [53/200], Train Loss: 0.0453, Val Loss: 0.0277\n",
      "Epoch [54/200], Train Loss: 0.0450, Val Loss: 0.0299\n",
      "Epoch [55/200], Train Loss: 0.0449, Val Loss: 0.0277\n",
      "Epoch [56/200], Train Loss: 0.0448, Val Loss: 0.0270\n",
      "Epoch [57/200], Train Loss: 0.0450, Val Loss: 0.0250\n",
      "Epoch [58/200], Train Loss: 0.0453, Val Loss: 0.0248\n",
      "Epoch [59/200], Train Loss: 0.0447, Val Loss: 0.0319\n",
      "Epoch [60/200], Train Loss: 0.0442, Val Loss: 0.0264\n",
      "Epoch [61/200], Train Loss: 0.0449, Val Loss: 0.0247\n",
      "Epoch [62/200], Train Loss: 0.0448, Val Loss: 0.0317\n",
      "Epoch [63/200], Train Loss: 0.0451, Val Loss: 0.0246\n",
      "Epoch [64/200], Train Loss: 0.0448, Val Loss: 0.0265\n",
      "Epoch [65/200], Train Loss: 0.0452, Val Loss: 0.0277\n",
      "Epoch [66/200], Train Loss: 0.0443, Val Loss: 0.0297\n",
      "Epoch [67/200], Train Loss: 0.0449, Val Loss: 0.0285\n",
      "Epoch [68/200], Train Loss: 0.0449, Val Loss: 0.0255\n",
      "Epoch [69/200], Train Loss: 0.0448, Val Loss: 0.0251\n",
      "Epoch [70/200], Train Loss: 0.0449, Val Loss: 0.0243\n",
      "Epoch [71/200], Train Loss: 0.0445, Val Loss: 0.0257\n",
      "Epoch [72/200], Train Loss: 0.0445, Val Loss: 0.0243\n",
      "Epoch [73/200], Train Loss: 0.0448, Val Loss: 0.0267\n",
      "Epoch [74/200], Train Loss: 0.0447, Val Loss: 0.0262\n",
      "Epoch [75/200], Train Loss: 0.0448, Val Loss: 0.0318\n",
      "Epoch [76/200], Train Loss: 0.0448, Val Loss: 0.0286\n",
      "Epoch [77/200], Train Loss: 0.0464, Val Loss: 0.0257\n",
      "Epoch [78/200], Train Loss: 0.0447, Val Loss: 0.0243\n",
      "Epoch [79/200], Train Loss: 0.0451, Val Loss: 0.0243\n",
      "Epoch [80/200], Train Loss: 0.0450, Val Loss: 0.0261\n",
      "Epoch [81/200], Train Loss: 0.0447, Val Loss: 0.0251\n",
      "Epoch [82/200], Train Loss: 0.0444, Val Loss: 0.0320\n",
      "Epoch [83/200], Train Loss: 0.0450, Val Loss: 0.0321\n",
      "Epoch [84/200], Train Loss: 0.0453, Val Loss: 0.0273\n",
      "Epoch [85/200], Train Loss: 0.0447, Val Loss: 0.0250\n",
      "Epoch [86/200], Train Loss: 0.0447, Val Loss: 0.0288\n",
      "Epoch [87/200], Train Loss: 0.0453, Val Loss: 0.0264\n",
      "Epoch [88/200], Train Loss: 0.0446, Val Loss: 0.0246\n",
      "Epoch [89/200], Train Loss: 0.0443, Val Loss: 0.0276\n",
      "Epoch [90/200], Train Loss: 0.0445, Val Loss: 0.0265\n",
      "Epoch [91/200], Train Loss: 0.0442, Val Loss: 0.0256\n",
      "Epoch [92/200], Train Loss: 0.0444, Val Loss: 0.0243\n",
      "Epoch [93/200], Train Loss: 0.0454, Val Loss: 0.0266\n",
      "Epoch [94/200], Train Loss: 0.0452, Val Loss: 0.0253\n",
      "Epoch [95/200], Train Loss: 0.0449, Val Loss: 0.0263\n",
      "Epoch [96/200], Train Loss: 0.0448, Val Loss: 0.0274\n",
      "Epoch [97/200], Train Loss: 0.0448, Val Loss: 0.0252\n",
      "Epoch [98/200], Train Loss: 0.0450, Val Loss: 0.0246\n",
      "Epoch [99/200], Train Loss: 0.0447, Val Loss: 0.0339\n",
      "Epoch [100/200], Train Loss: 0.0450, Val Loss: 0.0252\n",
      "Epoch [101/200], Train Loss: 0.0444, Val Loss: 0.0326\n",
      "Epoch [102/200], Train Loss: 0.0448, Val Loss: 0.0253\n",
      "Epoch [103/200], Train Loss: 0.0452, Val Loss: 0.0265\n",
      "Epoch [104/200], Train Loss: 0.0450, Val Loss: 0.0248\n",
      "Epoch [105/200], Train Loss: 0.0449, Val Loss: 0.0275\n",
      "Epoch [106/200], Train Loss: 0.0447, Val Loss: 0.0258\n",
      "Epoch [107/200], Train Loss: 0.0448, Val Loss: 0.0275\n",
      "Epoch [108/200], Train Loss: 0.0443, Val Loss: 0.0260\n",
      "Epoch [109/200], Train Loss: 0.0448, Val Loss: 0.0262\n",
      "Epoch [110/200], Train Loss: 0.0449, Val Loss: 0.0269\n",
      "Epoch [111/200], Train Loss: 0.0445, Val Loss: 0.0274\n",
      "Epoch [112/200], Train Loss: 0.0447, Val Loss: 0.0267\n",
      "Epoch [113/200], Train Loss: 0.0442, Val Loss: 0.0334\n",
      "Epoch [114/200], Train Loss: 0.0451, Val Loss: 0.0262\n",
      "Epoch [115/200], Train Loss: 0.0445, Val Loss: 0.0256\n",
      "Epoch [116/200], Train Loss: 0.0448, Val Loss: 0.0273\n",
      "Epoch [117/200], Train Loss: 0.0386, Val Loss: 0.0260\n",
      "Epoch [118/200], Train Loss: 0.0275, Val Loss: 0.0245\n",
      "Epoch [119/200], Train Loss: 0.0260, Val Loss: 0.0255\n",
      "Epoch [120/200], Train Loss: 0.0246, Val Loss: 0.0245\n",
      "Epoch [121/200], Train Loss: 0.0259, Val Loss: 0.0251\n",
      "Epoch [122/200], Train Loss: 0.0257, Val Loss: 0.0246\n",
      "Epoch [123/200], Train Loss: 0.0254, Val Loss: 0.0245\n",
      "Epoch [124/200], Train Loss: 0.0250, Val Loss: 0.0242\n",
      "Epoch [125/200], Train Loss: 0.0254, Val Loss: 0.0244\n",
      "Epoch [126/200], Train Loss: 0.0392, Val Loss: 0.0429\n",
      "Epoch [127/200], Train Loss: 0.0470, Val Loss: 0.0243\n",
      "Epoch [128/200], Train Loss: 0.0442, Val Loss: 0.0243\n",
      "Epoch [129/200], Train Loss: 0.0454, Val Loss: 0.0288\n",
      "Epoch [130/200], Train Loss: 0.0423, Val Loss: 0.0282\n",
      "Epoch [131/200], Train Loss: 0.0432, Val Loss: 0.0292\n",
      "Epoch [132/200], Train Loss: 0.0435, Val Loss: 0.0269\n",
      "Epoch [133/200], Train Loss: 0.0416, Val Loss: 0.0243\n",
      "Epoch [134/200], Train Loss: 0.0421, Val Loss: 0.0269\n",
      "Epoch [135/200], Train Loss: 0.0444, Val Loss: 0.0243\n",
      "Epoch [136/200], Train Loss: 0.0446, Val Loss: 0.0243\n",
      "Epoch [137/200], Train Loss: 0.0445, Val Loss: 0.0323\n",
      "Epoch [138/200], Train Loss: 0.0460, Val Loss: 0.0261\n",
      "Epoch [139/200], Train Loss: 0.0446, Val Loss: 0.0243\n",
      "Epoch [140/200], Train Loss: 0.0442, Val Loss: 0.0248\n",
      "Epoch [141/200], Train Loss: 0.0447, Val Loss: 0.0266\n",
      "Epoch [142/200], Train Loss: 0.0442, Val Loss: 0.0250\n",
      "Epoch [143/200], Train Loss: 0.0443, Val Loss: 0.0244\n",
      "Epoch [144/200], Train Loss: 0.0450, Val Loss: 0.0243\n",
      "Epoch [145/200], Train Loss: 0.0449, Val Loss: 0.0269\n",
      "Epoch [146/200], Train Loss: 0.0454, Val Loss: 0.0247\n",
      "Epoch [147/200], Train Loss: 0.0446, Val Loss: 0.0280\n",
      "Epoch [148/200], Train Loss: 0.0445, Val Loss: 0.0318\n",
      "Epoch [149/200], Train Loss: 0.0456, Val Loss: 0.0245\n",
      "Epoch [150/200], Train Loss: 0.0448, Val Loss: 0.0248\n",
      "Epoch [151/200], Train Loss: 0.0445, Val Loss: 0.0253\n",
      "Epoch [152/200], Train Loss: 0.0444, Val Loss: 0.0246\n",
      "Epoch [153/200], Train Loss: 0.0458, Val Loss: 0.0279\n",
      "Epoch [154/200], Train Loss: 0.0447, Val Loss: 0.0247\n",
      "Epoch [155/200], Train Loss: 0.0450, Val Loss: 0.0288\n",
      "Epoch [156/200], Train Loss: 0.0446, Val Loss: 0.0242\n",
      "Epoch [157/200], Train Loss: 0.0447, Val Loss: 0.0294\n",
      "Epoch [158/200], Train Loss: 0.0450, Val Loss: 0.0244\n",
      "Epoch [159/200], Train Loss: 0.0444, Val Loss: 0.0275\n",
      "Epoch [160/200], Train Loss: 0.0448, Val Loss: 0.0285\n",
      "Epoch [161/200], Train Loss: 0.0449, Val Loss: 0.0261\n",
      "Epoch [162/200], Train Loss: 0.0444, Val Loss: 0.0275\n",
      "Epoch [163/200], Train Loss: 0.0446, Val Loss: 0.0257\n",
      "Epoch [164/200], Train Loss: 0.0452, Val Loss: 0.0284\n",
      "Epoch [165/200], Train Loss: 0.0447, Val Loss: 0.0249\n",
      "Epoch [166/200], Train Loss: 0.0446, Val Loss: 0.0259\n",
      "Epoch [167/200], Train Loss: 0.0451, Val Loss: 0.0246\n",
      "Epoch [168/200], Train Loss: 0.0450, Val Loss: 0.0258\n",
      "Epoch [169/200], Train Loss: 0.0449, Val Loss: 0.0286\n",
      "Epoch [170/200], Train Loss: 0.0438, Val Loss: 0.0244\n",
      "Epoch [171/200], Train Loss: 0.0445, Val Loss: 0.0256\n",
      "Epoch [172/200], Train Loss: 0.0443, Val Loss: 0.0274\n",
      "Epoch [173/200], Train Loss: 0.0445, Val Loss: 0.0243\n",
      "Epoch [174/200], Train Loss: 0.0447, Val Loss: 0.0261\n",
      "Epoch [175/200], Train Loss: 0.0445, Val Loss: 0.0271\n",
      "Epoch [176/200], Train Loss: 0.0445, Val Loss: 0.0243\n",
      "Epoch [177/200], Train Loss: 0.0447, Val Loss: 0.0316\n",
      "Epoch [178/200], Train Loss: 0.0446, Val Loss: 0.0244\n",
      "Epoch [179/200], Train Loss: 0.0445, Val Loss: 0.0288\n",
      "Epoch [180/200], Train Loss: 0.0446, Val Loss: 0.0267\n",
      "Epoch [181/200], Train Loss: 0.0440, Val Loss: 0.0263\n",
      "Epoch [182/200], Train Loss: 0.0448, Val Loss: 0.0284\n",
      "Epoch [183/200], Train Loss: 0.0449, Val Loss: 0.0260\n",
      "Epoch [184/200], Train Loss: 0.0442, Val Loss: 0.0257\n",
      "Epoch [185/200], Train Loss: 0.0455, Val Loss: 0.0261\n",
      "Epoch [186/200], Train Loss: 0.0448, Val Loss: 0.0265\n",
      "Epoch [187/200], Train Loss: 0.0454, Val Loss: 0.0273\n",
      "Epoch [188/200], Train Loss: 0.0453, Val Loss: 0.0286\n",
      "Epoch [189/200], Train Loss: 0.0453, Val Loss: 0.0282\n",
      "Epoch [190/200], Train Loss: 0.0447, Val Loss: 0.0256\n",
      "Epoch [191/200], Train Loss: 0.0445, Val Loss: 0.0255\n",
      "Epoch [192/200], Train Loss: 0.0443, Val Loss: 0.0270\n",
      "Epoch [193/200], Train Loss: 0.0446, Val Loss: 0.0269\n",
      "Epoch [194/200], Train Loss: 0.0450, Val Loss: 0.0282\n",
      "Epoch [195/200], Train Loss: 0.0447, Val Loss: 0.0247\n",
      "Epoch [196/200], Train Loss: 0.0442, Val Loss: 0.0250\n",
      "Epoch [197/200], Train Loss: 0.0448, Val Loss: 0.0281\n",
      "Epoch [198/200], Train Loss: 0.0447, Val Loss: 0.0261\n",
      "Epoch [199/200], Train Loss: 0.0443, Val Loss: 0.0260\n",
      "Epoch [200/200], Train Loss: 0.0447, Val Loss: 0.0265\n",
      "RMSE: 0.1601\n",
      "MAPE: 9.39%\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "input_dim = 3  # 각 데이터 포인트의 차원\n",
    "hidden_dim = 16\n",
    "num_layers = 8\n",
    "dropout = 0.1\n",
    "num_epochs = 200\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 모델 초기화\n",
    "model = LSTMRegressionModel(input_dim, hidden_dim, num_layers, dropout)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 학습\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "# 평가\n",
    "rmse, mape = evaluate_model(model, val_loader, device)\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "liion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
